{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Fake News Analizer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps\n",
    "\n",
    " Here we import all the libraries needed for the project and import the data from the CSV. All the data is in one csv named \"fake.csv\". This csv has got several rows and columns that will be evaluated in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import nltk\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       uuid  ord_in_thread  \\\n",
      "0  6a175f46bcd24d39b3e962ad0f29936721db70db              0   \n",
      "1  2bdc29d12605ef9cf3f09f9875040a7113be5d5b              0   \n",
      "2  c70e149fdd53de5e61c29281100b9de0ed268bc3              0   \n",
      "3  7cf7c15731ac2a116dd7f629bd57ea468ed70284              0   \n",
      "4  0206b54719c7e241ffe0ad4315b808290dbe6c0f              0   \n",
      "\n",
      "                 author                      published  \\\n",
      "0     Barracuda Brigade  2016-10-26T21:41:00.000+03:00   \n",
      "1  reasoning with facts  2016-10-29T08:47:11.259+03:00   \n",
      "2     Barracuda Brigade  2016-10-31T01:41:49.479+02:00   \n",
      "3                Fed Up  2016-11-01T05:22:00.000+02:00   \n",
      "4                Fed Up  2016-11-01T21:56:00.000+02:00   \n",
      "\n",
      "                                               title  \\\n",
      "0  Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
      "1  Re: Why Did Attorney General Loretta Lynch Ple...   \n",
      "2  BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
      "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
      "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
      "\n",
      "                                                text language  \\\n",
      "0  Print They should pay all the back all the mon...  english   \n",
      "1  Why Did Attorney General Loretta Lynch Plead T...  english   \n",
      "2  Red State : \\nFox News Sunday reported this mo...  english   \n",
      "3  Email Kayla Mueller was a prisoner and torture...  english   \n",
      "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  english   \n",
      "\n",
      "                         crawled             site_url country  domain_rank  \\\n",
      "0  2016-10-27T01:49:27.168+03:00  100percentfedup.com      US      25689.0   \n",
      "1  2016-10-29T08:47:11.259+03:00  100percentfedup.com      US      25689.0   \n",
      "2  2016-10-31T01:41:49.479+02:00  100percentfedup.com      US      25689.0   \n",
      "3  2016-11-01T15:46:26.304+02:00  100percentfedup.com      US      25689.0   \n",
      "4  2016-11-01T23:59:42.266+02:00  100percentfedup.com      US      25689.0   \n",
      "\n",
      "                                        thread_title  spam_score  \\\n",
      "0  Muslims BUSTED: They Stole Millions In Gov’t B...       0.000   \n",
      "1  Re: Why Did Attorney General Loretta Lynch Ple...       0.000   \n",
      "2  BREAKING: Weiner Cooperating With FBI On Hilla...       0.000   \n",
      "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...       0.068   \n",
      "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...       0.865   \n",
      "\n",
      "                                        main_img_url  replies_count  \\\n",
      "0  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
      "1  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
      "2  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
      "3  http://100percentfedup.com/wp-content/uploads/...              0   \n",
      "4  http://100percentfedup.com/wp-content/uploads/...              0   \n",
      "\n",
      "   participants_count  likes  comments  shares  type  \n",
      "0                   1      0         0       0  bias  \n",
      "1                   1      0         0       0  bias  \n",
      "2                   1      0         0       0  bias  \n",
      "3                   0      0         0       0  bias  \n",
      "4                   0      0         0       0  bias  \n"
     ]
    }
   ],
   "source": [
    "fake = pd.read_csv(\"./archive/fake.csv\", na_values=[\"\"])\n",
    "print(fake.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see it is a pretty big dataset. Let's talk numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  12999\n",
      "Number of columns:  20\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows: \", len(fake.index))\n",
    "\n",
    "print(\"Number of columns: \", len(fake.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12999 rows and 20 different columns. This constitutes a big sample of fake news to analyse. Let's now see which type of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uuid                   object\n",
      "ord_in_thread           int64\n",
      "author                 object\n",
      "published              object\n",
      "title                  object\n",
      "text                   object\n",
      "language               object\n",
      "crawled                object\n",
      "site_url               object\n",
      "country                object\n",
      "domain_rank           float64\n",
      "thread_title           object\n",
      "spam_score            float64\n",
      "main_img_url           object\n",
      "replies_count           int64\n",
      "participants_count      int64\n",
      "likes                   int64\n",
      "comments                int64\n",
      "shares                  int64\n",
      "type                   object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(fake.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Let's start by removing any data duplicates that add nothing to the dataset. We should compare the number of rows before and after removing the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before:  12999\n",
      "Number of rows after:  12999\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake.drop_duplicates()\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that there are no duplicate rows. Moving on to missing data.\n",
    "\n",
    "We should find all the rows with missing data and acknowledge every missing information column-wise. Therefore, we should see check for each column the missing information and see if we should either remove the collumn totally or find a viable substitution for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_rank           4223\n",
      "main_img_url          3643\n",
      "author                2424\n",
      "title                  680\n",
      "country                176\n",
      "text                    46\n",
      "thread_title            12\n",
      "shares                   0\n",
      "comments                 0\n",
      "likes                    0\n",
      "participants_count       0\n",
      "replies_count            0\n",
      "uuid                     0\n",
      "spam_score               0\n",
      "ord_in_thread            0\n",
      "site_url                 0\n",
      "crawled                  0\n",
      "language                 0\n",
      "published                0\n",
      "type                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(fake.isnull().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a descending order of the number of missing values per collumn. We will now analyse each column and study if it is worth \"fixing\" or substituting the missing values or just delete the column all together.\n",
    "\n",
    "As we can see, we have a column named \"main_img_url\" that doesn't provide any useful data for the study of this dataset. Because of that, we decided to remove it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.drop(['main_img_url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the column \"author\" there are two different cases that caught our attention: there are \"anonymous\" authors and just missing authors. Since both of these cases are comparable, because there is no info about the author in neither of them, we decided to make them all the same and add \"anonymous\" to the rows where the \"author\" info is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12999 entries, 0 to 12998\n",
      "Data columns (total 19 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   uuid                12999 non-null  object \n",
      " 1   ord_in_thread       12999 non-null  int64  \n",
      " 2   author              12999 non-null  object \n",
      " 3   published           12999 non-null  object \n",
      " 4   title               12319 non-null  object \n",
      " 5   text                12953 non-null  object \n",
      " 6   language            12999 non-null  object \n",
      " 7   crawled             12999 non-null  object \n",
      " 8   site_url            12999 non-null  object \n",
      " 9   country             12823 non-null  object \n",
      " 10  domain_rank         8776 non-null   float64\n",
      " 11  thread_title        12987 non-null  object \n",
      " 12  spam_score          12999 non-null  float64\n",
      " 13  replies_count       12999 non-null  int64  \n",
      " 14  participants_count  12999 non-null  int64  \n",
      " 15  likes               12999 non-null  int64  \n",
      " 16  comments            12999 non-null  int64  \n",
      " 17  shares              12999 non-null  int64  \n",
      " 18  type                12999 non-null  object \n",
      "dtypes: float64(2), int64(6), object(11)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "fake[\"author\"]=fake[\"author\"].fillna(\"Anonymous\")\n",
    "fake.to_csv(\"fake_2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73c5f639fc5f17e81c041bf2859659ce632f9601e0bd6e44c786cae6c9bf7ef3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
