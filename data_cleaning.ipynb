{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Fake News Analyzer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps\n",
    "\n",
    " Here we import all the libraries needed for the project and import the data from the CSV. All the data is in one csv named \"fake.csv\". This csv has got several rows and columns that will be evaluated in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> FAKE NEWS ANALIZER <-\n",
      "                                       uuid  ord_in_thread  \\\n",
      "0  6a175f46bcd24d39b3e962ad0f29936721db70db              0   \n",
      "1  2bdc29d12605ef9cf3f09f9875040a7113be5d5b              0   \n",
      "2  c70e149fdd53de5e61c29281100b9de0ed268bc3              0   \n",
      "3  7cf7c15731ac2a116dd7f629bd57ea468ed70284              0   \n",
      "4  0206b54719c7e241ffe0ad4315b808290dbe6c0f              0   \n",
      "\n",
      "                 author                      published  \\\n",
      "0     Barracuda Brigade  2016-10-26T21:41:00.000+03:00   \n",
      "1  reasoning with facts  2016-10-29T08:47:11.259+03:00   \n",
      "2     Barracuda Brigade  2016-10-31T01:41:49.479+02:00   \n",
      "3                Fed Up  2016-11-01T05:22:00.000+02:00   \n",
      "4                Fed Up  2016-11-01T21:56:00.000+02:00   \n",
      "\n",
      "                                               title  \\\n",
      "0  Muslims BUSTED: They Stole Millions In Gov’t B...   \n",
      "1  Re: Why Did Attorney General Loretta Lynch Ple...   \n",
      "2  BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
      "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
      "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
      "\n",
      "                                                text language  \\\n",
      "0  Print They should pay all the back all the mon...  english   \n",
      "1  Why Did Attorney General Loretta Lynch Plead T...  english   \n",
      "2  Red State : \\nFox News Sunday reported this mo...  english   \n",
      "3  Email Kayla Mueller was a prisoner and torture...  english   \n",
      "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  english   \n",
      "\n",
      "                         crawled             site_url country  domain_rank  \\\n",
      "0  2016-10-27T01:49:27.168+03:00  100percentfedup.com      US      25689.0   \n",
      "1  2016-10-29T08:47:11.259+03:00  100percentfedup.com      US      25689.0   \n",
      "2  2016-10-31T01:41:49.479+02:00  100percentfedup.com      US      25689.0   \n",
      "3  2016-11-01T15:46:26.304+02:00  100percentfedup.com      US      25689.0   \n",
      "4  2016-11-01T23:59:42.266+02:00  100percentfedup.com      US      25689.0   \n",
      "\n",
      "                                        thread_title  spam_score  \\\n",
      "0  Muslims BUSTED: They Stole Millions In Gov’t B...       0.000   \n",
      "1  Re: Why Did Attorney General Loretta Lynch Ple...       0.000   \n",
      "2  BREAKING: Weiner Cooperating With FBI On Hilla...       0.000   \n",
      "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...       0.068   \n",
      "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...       0.865   \n",
      "\n",
      "                                        main_img_url  replies_count  \\\n",
      "0  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
      "1  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
      "2  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
      "3  http://100percentfedup.com/wp-content/uploads/...              0   \n",
      "4  http://100percentfedup.com/wp-content/uploads/...              0   \n",
      "\n",
      "   participants_count  likes  comments  shares  type  \n",
      "0                   1      0         0       0  bias  \n",
      "1                   1      0         0       0  bias  \n",
      "2                   1      0         0       0  bias  \n",
      "3                   0      0         0       0  bias  \n",
      "4                   0      0         0       0  bias  \n"
     ]
    }
   ],
   "source": [
    "#imports and reading dataset\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "print(\"-> FAKE NEWS ANALYZER <-\")\n",
    "\n",
    "\n",
    "fake = pd.read_csv(\"./archive/fake.csv\", na_values=[\"\"])\n",
    "print(fake.head(n=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see it is a pretty big dataset. Let's talk numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  12999\n",
      "Number of columns:  20\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows: \", len(fake.index))\n",
    "\n",
    "print(\"Number of columns: \", len(fake.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12999 rows and 20 different columns. This constitutes a big sample of fake news to analyse. Let's now see which type of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Printing collumn's data types <-\n",
      "uuid                   object\n",
      "ord_in_thread           int64\n",
      "author                 object\n",
      "published              object\n",
      "title                  object\n",
      "text                   object\n",
      "language               object\n",
      "crawled                object\n",
      "site_url               object\n",
      "country                object\n",
      "domain_rank           float64\n",
      "thread_title           object\n",
      "spam_score            float64\n",
      "main_img_url           object\n",
      "replies_count           int64\n",
      "participants_count      int64\n",
      "likes                   int64\n",
      "comments                int64\n",
      "shares                  int64\n",
      "type                   object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Printing collumn's data types <-\")\n",
    "\n",
    "print(fake.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Duplicates\n",
    "\n",
    "Let's start by removing any data duplicates that add nothing to the dataset. We should compare the number of rows before and after removing the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> DATA CLEANING <-\n",
      "-> Dropping duplicate rows <-\n",
      "Number of rows before:  12999\n",
      "Number of rows after:  12999\n"
     ]
    }
   ],
   "source": [
    "print(\"-> DATA CLEANING <-\")\n",
    "\n",
    "print(\"-> Dropping duplicate rows <-\")\n",
    "\n",
    "#cleaning duplicates\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake.drop_duplicates()\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that there are no duplicate rows. Moving on to missing data.\n",
    "\n",
    "## Missing data\n",
    "\n",
    "We should find all the rows with missing data and acknowledge every missing information column-wise. Therefore, we should see check for each column the missing information and see if we should either remove the collumn totally or find a viable substitution for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Showing null values per collumn - before cleaning <-\n",
      "domain_rank           4223\n",
      "main_img_url          3643\n",
      "author                2424\n",
      "title                  680\n",
      "country                176\n",
      "text                    46\n",
      "thread_title            12\n",
      "shares                   0\n",
      "comments                 0\n",
      "likes                    0\n",
      "participants_count       0\n",
      "replies_count            0\n",
      "uuid                     0\n",
      "spam_score               0\n",
      "ord_in_thread            0\n",
      "site_url                 0\n",
      "crawled                  0\n",
      "language                 0\n",
      "published                0\n",
      "type                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Showing null values per collumn - before cleaning <-\")\n",
    "\n",
    "print(fake.isnull().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a descending order of the number of missing values per collumn. We will now analyse each column and study if it is worth \"fixing\" or substituting the missing values or just delete the column all together.\n",
    "\n",
    "As we can see, we have a column named \"main_img_url\" that doesn't provide any useful data for the study of this dataset. Because of that, we decided to remove it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Dropping 'main_img_url' collumn <-\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Dropping 'main_img_url' collumn <-\")\n",
    "\n",
    "fake.drop(['main_img_url'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the column \"author\" there are two different cases that caught our attention: there are \"anonymous\" authors and just missing authors. Since both of these cases are comparable, because there is no info about the author in neither of them, we decided to make them all the same and add \"anonymous\" to the rows where the \"author\" info is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Fixing null values in 'author' <-\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Fixing null values in 'author' <-\")\n",
    "\n",
    "fake[\"author\"]=fake[\"author\"].fillna(\"Anonymous\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can also see, there are 12 rows without a thread title. This doesn't allow us group up the fake news into threads because we don't know where they belong. Since it is such a small ammount of news (12) we decided that they should removed as they don't represent a big sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Fixing null values in 'thread_title' <-\n",
      "Number of rows before:  12999\n",
      "Number of rows after:  12987\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Fixing null values in 'thread_title' <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['thread_title'].notna()]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to evaluate and perform text search based on the text of the news, news with no text become irrelevant to this dataset. Therefore, we are going to remove those rows as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Fixing null values in 'text' <-\n",
      "Number of rows before:  12987\n",
      "Number of rows after:  12941\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Fixing null values in 'text' <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['text'].notna()]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are news with no title which we have found to be in the same thread where only the news in first place (order_in_thread=0) has got a title. Let's verify if all the existing titles are the same as the thread_title associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique titles: 11653\n",
      "Number of unique thread_titles: 11742 \n",
      "\n",
      "Different thread_title: Did Google Kill Julian Assange?\n",
      "Different thread_title: Prominent Democrat Connected To Clintons Donated $675,000 To Campaign Of Deputy FBI Director's Wife\n",
      "Different thread_title: Is it coming into clearer focus for Americans, or not?\n",
      "Different thread_title: Weiner Sings Like a Bird as Dickileak Provides Exit as Huma Sequestered by FBI at MOUNT WEATHER - After Huma's 65,000 Emails Found...No Longer Together\n",
      "Different thread_title: Installing a President by Force: Hillary Clinton and Our Moribund Democracy\n",
      "Different thread_title: Clinton's Policies Look Like a Death Sentence for Americans - American Herald Tribune\n",
      "Different thread_title: is bed the EU made, now it must lie in it -\n",
      "Different thread_title: Justin Trudeau Doesn't Tell the Truth to Canadians about Syria\n",
      "Different thread_title: 10,000 children are missing and the world just looks away\n",
      "Different thread_title: The Orlando Shooting and our reptilian minds\n",
      "Different thread_title: Anonymous: Bill Clinton Pedophile Sex Tape Filmed By Jeffrey Epstein!\n",
      "Different thread_title: Fears Grow Julian Assange Was Extradited On ‘Guantanamo Express’\n",
      "Different thread_title: Promoting Peace: A Large Number of Israeli and Palestinian Women March Together\n",
      "Different thread_title: Surgeons Admit Mammography is Outdated, Harmful at Best\n",
      "Different thread_title: New Report Reveals Modern Diets are Making People Sicker Across the World\n",
      "Different thread_title: Cheap Wind-Powered Device Produces 11 Gallons of Clean Drinking Water Per Day from the Air\n",
      "Different thread_title: Everything You Knew About the Paleo Diet is Wrong. Here’s the Proof\n",
      "Different thread_title: Barack Obama ‘Reveals’: How NASA Will Send Humans To Mars By The 2030s\n",
      "Different thread_title: Wisconsin Supreme Court Guts Fourth Amendment in Unprecedented Decision\n",
      "Different thread_title: Ecuador Admits They Silenced Assange Because Clinton Leaks Were ‘Interfering’ With US Election\n",
      "Different thread_title: The History of the House of Rothschild (1743-2006)\n",
      "Different thread_title: Crash Victim Calls for Help - Gets Arrested and Sexually Assaulted Instead\n",
      "Different thread_title: Pennsylvania Pipelines Bursts, Leaks 55,000 Gallons Of Gas Into One Of US’ Most Endangered Rivers\n",
      "Different thread_title: Crash Victim Calls Police for Help - Gets Arrested and Sexually Assaulted Instead\n",
      "Different thread_title: Join Us Worldwide for the 2016 Million Mask March!\n",
      "Different thread_title: 7 Insidious Ways the Elite Are Making Americans Dumber (Video) | Power Elite\n",
      "Different thread_title: Clinton & Soros Bribing the Electoral College Voters-Massive Voter Fraud in AZ | Conspiracy Theories\n",
      "Different thread_title: Analysis: Election Outcome Scenarios Reveal 95% Chance of Widespread Post-Election Violence… Streets of America to Run Red With Blood | War and Conflict\n",
      "Different thread_title: This Is Definitely Pissing a Lot of Hillary People Off! | Alternative\n",
      "Different thread_title: X22Report Shadow Banking Is Back, and This Time There Is No Hope - Episode 1110a | Politics\n",
      "Different thread_title: Not Without the Consent of the Governed | Police State\n",
      "Different thread_title: The Mothership Arrives --- But It's Not What You Think! | Alternative\n",
      "Different thread_title: Greg Hunter, \"USA Actually Bankrupt Now\" | Opinion - Liberal\n",
      "Different thread_title: Paul Craig Roberts: ‘By Cooperating with Washington on Syria Russia Walked Into a Trap’ | Alternative\n",
      "Different thread_title: Breaking! Wikileaks to Take Down Top Clinton Strategist (Video) | Prophecy\n",
      "Different thread_title: Red Warning – Nuclear Attack Russian Surprise for US Military – Surrounded USA NATO Military Drills on Russian Border – Putin Issues a Ultimatum to the United States | Self-Sufficiency\n",
      "Different thread_title: WATCH: Muslim Worshipers Mistake Synagogue For Mosque\n",
      "Different thread_title: California Politicians Demand National Guard Forgive Enlistment Bonuses\n",
      "Different thread_title: 2 Somali Immigrants Guilty of Aiding Foreign Terrorists\n",
      "Different thread_title: Duterte: Philippines Will Not Be a 'Dog Barking for Crumbs' from U.S.\n",
      "Different thread_title: Report: Islamic State Incinerates Jihadis in Burning Oil for Fleeing Mosul Battle\n",
      "Different thread_title: Islamic State Terrorists 'Shave Beards', Prepare to Flee Mosul\n",
      "Different thread_title: UNESCO Approves Motion Denying Jewish, Christian Links to Temple Mount\n",
      "Different thread_title: Iran Unveils Attack 'Suicide Drone'\n",
      "Different thread_title: Pentagon Warns Chinese Computer Parts May Compromise Cyber-Security\n",
      "Different thread_title: U.S. Pledges Another $800 Million to Afghanistan\n",
      "Different thread_title: Russia TV Promoting Nuclear War Drills as U.S. Deploys Troops to Norway\n",
      "Different thread_title: Women Love to Objectify Themselves: Here’s Why\n",
      "Different thread_title: “Pedophiles are the New Jews”\n",
      "Different thread_title: Ellen Degeneres: Quits Talk Show to Save Portia De Rossi Marriage?\n",
      "Different thread_title: Tumbler \"Come to Edge\" Underground Examinations Music Review\n",
      "Different thread_title: Nelson Mandela’s Top Five Contributions to Humanity\n",
      "Different thread_title: Garth Brooks Sings About Divorce from Trisha in 'The Call'\n",
      "Different thread_title: NASA Confirmed Alien Invasion?\n",
      "Different thread_title: NASA Weighs in on Apocalyptic Sounds From Heaven Heard During Month of Pentecost and Shavuot [Videos]\n",
      "Different thread_title: NASA Confirms -Super Human Abilities Gained\n",
      "Different thread_title: Is Mel Gibson on Steroids?\n",
      "Different thread_title: Christianity and Liberalism Cannot Exist Together\n",
      "Different thread_title: The Double Standards of American Politics (What Do I Tell My Daughter?)\n",
      "Different thread_title: Could Real Life 'Purge' Threat Happen This Weekend?\n",
      "Different thread_title: HPV: The Quiet Virus That Causes Very Loud Problems\n",
      "Different thread_title: KKK and Racist Groups Have Killed More People Than Islamist Terrorists in the U.S. Since 9/11\n",
      "Different thread_title: Television: Highest-Paid Actors and Actresses to Date\n",
      "Different thread_title: Celebrities Why Do They Have so Much Influence?\n",
      "Different thread_title: Mel Gibson Starring in 'Blood Father'\n",
      "Different thread_title: Battlestar Galactica Gets Another Remake\n",
      "Different thread_title: WHERES THERE A CRASH? WORLD ECONOMY COLLAPSING!!! - HEADLINES OCTOBER 2016\n",
      "Different thread_title: Time To Panic About Deutsche Bank & Credit Suisse! - Probably The Next Lehman\n",
      "Different thread_title: Important Vid Fukushima I Nuclear Power Plant\n",
      "Different thread_title: HACKING ATTACKS\n",
      "Different thread_title: WINDOWS 10\n",
      "Different thread_title: GOOGLE - ALPHABET\n",
      "Different thread_title: APPLE\n",
      "Different thread_title: Communism, fascism, nazism, Marxism ALL created by the Vatican not Jews\n",
      "Different thread_title: Hitler hated the Slavs more than a Jews ?\n",
      "Different thread_title: How Do We Get to a Conversation in This Country About Climate?\n",
      "Different thread_title: Obamacare Premiums Are Set to Increase Steeply Next Year\n",
      "Different thread_title: One of the Biggest Media Mergers Ever May Be on the Horizon\n",
      "Different thread_title: Five Terrifying Things From Trump's Blueprint for His First 100 Days if Elected\n",
      "Different thread_title: Unforeseen Consequences: The Death of Trees\n",
      "Different thread_title: Donald Trump Could Have Manufactured His Products in the US; He Chose China\n",
      "Different thread_title: \"We've Never Seen the Government Stand Up to the Fossil Fuel Industry\": Tim DeChristopher on Our Climate Future\n",
      "Different thread_title: Donald Trump's Star On Hollywood Walk Of Fame Vandalized\n",
      "Different thread_title: Judge Files Criminal Charge Against Sheriff Joe Arpaio For Enforcing Federal Immigration Laws\n",
      "Different thread_title: Independent Voters Push Trump To The Front In Florida And Ohio\n",
      "Different thread_title: O'Reilly Grills Washington Post Columnist Who Says He Is Not A 'Real' News Person\n",
      "Different thread_title: Same-Sex Marriage And The Politics Of Aggression\n",
      "Different thread_title: Hannity Proposes A Sendoff For Obama In The Event Of A Trump Presidency\n",
      "Different thread_title: Caught On Tape: ISIS Destroys Abrams Tank With Anti-Tank Missile\n",
      "Different thread_title: Tesla Earnings Smash Expectations After Dramatic Change In Reporting Methodology\n"
     ]
    }
   ],
   "source": [
    "# get unique values from each column\n",
    "titles = fake['title'].unique()\n",
    "thread_titles = fake['thread_title'].unique()\n",
    "\n",
    "#check if size is the same\n",
    "print(\"Number of unique titles:\", len(titles))\n",
    "print(\"Number of unique thread_titles:\", len(thread_titles),\"\\n\")\n",
    "\n",
    "# convert every value to string\n",
    "for i in range(0, len(titles)):\n",
    "    titles[i]=str(titles[i])\n",
    "\n",
    "for i in range(0, len(thread_titles)):\n",
    "    thread_titles[i] = str(thread_titles[i])\n",
    "\n",
    "#sort arrays\n",
    "np.sort(titles)\n",
    "np.sort(thread_titles)\n",
    "\n",
    "\n",
    "# print every different title - compare to bigger list: thread_titles\n",
    "for element in thread_titles:\n",
    "    if(element not in titles):\n",
    "        print(\"Different thread_title:\", element)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are different titles and thread_titles, making it impossible for us to eliminate one of those columns. We can, however, substitute the data and eliminate rows which don't have any of these informations, as it is impossible for us to track the news associated to any theme or key word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with no title and no thread_title: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows with no title and no thread_title:\", len(fake.loc[fake['title'].isna() & fake['thread_title'].isna()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no rows with neither of the values in the referred columns, we should turn our attention to how to correct the missing data in these columns. For every row with no title, we will substitute the value with the thread_title instead, since the thread_title will be in some way connected to the theme of the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Fixing null values in 'title' <-\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Fixing null values in 'title' <-\")\n",
    "\n",
    "fake.title.fillna(fake.thread_title, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, there are news with no 'country' value associated, more specifically, 176 news. To make it easier to analyze this data and not have missing values, we will drop these rows, since we can't track the origin of the news and it is a rather small ammount of data that is discarded (176 rows), given the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Fixing null values in 'country' <-\n",
      "Number of rows before:  12941\n",
      "Number of rows after:  12765\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Fixing null values in 'country' <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['country'].notna()]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As domain rank is a metric, going from 0 to 100, the later being the strongest, evaluated based on many factors dependent on user searches and the authority of a certain domain, if a domain doesn't have an available ranking, we can suppose it is 0 since it hasn't been evaluated yet. That can be due to lack of information or visits to that domain. That being said, we will substitute every missing value in the column \"domain_rank\" by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Fixing null values in 'domain_rank' <-\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Fixing null values in 'domain_rank' <-\")\n",
    "\n",
    "fake[\"domain_rank\"] = fake[\"domain_rank\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a new look at the null values per column and see the final dataset, with no missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Showing null values per collumn - after cleaning <-\n",
      "uuid                  0\n",
      "domain_rank           0\n",
      "shares                0\n",
      "comments              0\n",
      "likes                 0\n",
      "participants_count    0\n",
      "replies_count         0\n",
      "spam_score            0\n",
      "thread_title          0\n",
      "country               0\n",
      "ord_in_thread         0\n",
      "site_url              0\n",
      "crawled               0\n",
      "language              0\n",
      "text                  0\n",
      "title                 0\n",
      "published             0\n",
      "author                0\n",
      "type                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Showing null values per collumn - after cleaning <-\")\n",
    "\n",
    "print(fake.isnull().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we have observed that uuid column doesn't really add anything of value to our dataset, therefore, we will drop the column all together before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Dropping 'uuid' collumn <-\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Dropping 'uuid' collumn <-\")\n",
    "\n",
    "fake.drop(['uuid'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broken Data & Data Types\n",
    "\n",
    "As in all datasets, there are rows with broken data, as in, data that was unsuccessfully crawled and filled in rows with broken values. Let's find these rows in our dataset and correct or drop them.\n",
    "\n",
    "The first case we noticed were some rows filled with 0 and 1, namely in the title and text columns. Let's locate them and print them, to analize the data and see if it should be dropped. If we can't find any and since we already cleaned some of the data, it means they were already dropped in the steps before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning crawling errors <-\n",
      "Empty DataFrame\n",
      "Columns: [ord_in_thread, author, published, title, text, language, crawled, site_url, country, domain_rank, thread_title, spam_score, replies_count, participants_count, likes, comments, shares, type]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [ord_in_thread, author, published, title, text, language, crawled, site_url, country, domain_rank, thread_title, spam_score, replies_count, participants_count, likes, comments, shares, type]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [ord_in_thread, author, published, title, text, language, crawled, site_url, country, domain_rank, thread_title, spam_score, replies_count, participants_count, likes, comments, shares, type]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [ord_in_thread, author, published, title, text, language, crawled, site_url, country, domain_rank, thread_title, spam_score, replies_count, participants_count, likes, comments, shares, type]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning crawling errors <-\")\n",
    "\n",
    "print(fake.loc[fake['title'] == \"0\"])\n",
    "print(fake.loc[fake['title'] == \"1\"])\n",
    "\n",
    "print(fake.loc[fake['text'] == \"0\"])\n",
    "print(fake.loc[fake['text'] == \"1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional verification that the above statement was correct, we opened the dataset in excel and verified that every case of broken data was indeed gone from the dataset at this point of the cleaning.\n",
    "\n",
    "The other big case for broken data that we found were some news that weren't correctly crawled and just extended the text of the news throughout several columns. There are several ways of correcting this issue, but in order to get all the cases and clean the data the most, we will start by verifying column by column if the data types and formats are correct, dropping the columns in which that isn't verified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with 'ord_in_thread' column. It must be a number, therefore let's eliminate every row that doesn't just contain a number. If there is a row with alpha characters in it, the type of the column won't be int64 so let's first verify the type and eliminate non numeric rows if it isn't 'int64'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'ord_in_thread' collumn <-\n",
      "Number of rows before:  12765\n",
      "Type of 'ord_in_thread': int64\n",
      "Is it int64? True\n",
      "Number of rows after:  12765\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'ord_in_thread' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'ord_in_thread':\", fake['ord_in_thread'].dtypes)\n",
    "print(\"Is it int64?\", fake['ord_in_thread'].dtypes == 'int64')\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it returns 'int64' we are sure that the whole column only contains numeric values. Moving on to 'author', there are 2 errors to fix: wrong datatypes and the \"-NO AUTHOR-\" rows. As it is very hard to verify the correctness of the author name, as it is very hard to define \"correctness\" in these cases, and not exclude valid names with numeric digits using the usual rules for identifying names (since authors can have numbers in their username), we will move on to correcting the \"-NO AUTHOR-\" cases, where it should be replaced by \"Anonymous\", as we did before with null values. We will also remove rows with just whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'author' collumn <-\n",
      "Number of rows before:  12765\n",
      "Number of rows after:  12765\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'author' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake[\"author\"] = fake[\"author\"].replace(\"-NO AUTHOR-\",\"Anonymous\")\n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['author'].str.contains(r'^\\s*$')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we have the column \"published\", that should only contain dates. We can verify this through a regex expression that verifies the format of the dates, as well as the datatypes present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'published' collumn <-\n",
      "Number of rows before:  12765\n",
      "Number of rows after:  12765\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'published' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake=fake[fake['published'].str.contains('\\d{4}-\\d{2}-\\d{2}')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next column to verify is the column \"title\". We have cleaned this column before so some work was already done. We should just verify that the titles aren't mere strings of whitespaces, as it is extremely difficult to define a \"correct\" title or text in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'title' collumn <-\n",
      "Number of rows before:  12765\n",
      "Number of rows after:  12765\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'title' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['title'].str.contains(r'^\\s*$')]\n",
    "\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the \"text\" column, we must do the same verification as in the \"title\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'text' collumn <-\n",
      "Number of rows before:  12765\n",
      "Number of rows after:  12657\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'text' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[~fake['text'].str.contains(r'^\\s*$')]  #clean string rows with just whitespace\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we have the \"language\" column. This column must only contain one word and no digits, so the funtion isalpha() will do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'language' collumn <-\n",
      "Number of rows before:  12657\n",
      "Number of rows after:  12657\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'language' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake['language'] = fake['language'].apply(str)\n",
    "fake = fake[fake.language.str.isalpha()] \n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['language'].str.contains(r'^\\s*$')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the column \"crawled\", which should only contain the date in which the data was crawled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'crawled' collumn <-\n",
      "Number of rows before:  12657\n",
      "Number of rows after:  12657\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'crawled' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['crawled'].str.contains(r'\\d{4}-\\d{2}-\\d{2}')]  # dates\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following column is \"site_url\". Every site follows the same structure of domain.xxx, so we will apply a regex expression to filter unwanted formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'site_url' collumn <-\n",
      "Number of rows before:  12657\n",
      "Number of rows after:  12657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shit\\AppData\\Local\\Temp/ipykernel_9692/1779741962.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  fake = fake[fake['site_url'].str.contains(r'^(www\\.)?(.*)?\\/?(.)*')]\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'site_url' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['site_url'].str.contains(r'^(www\\.)?(.*)?\\/?(.)*')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the 'country' column, the countries are mentioned as a 2 uppercase letter code. Let's confirm every row follows that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'country' collumn <-\n",
      "Number of rows before:  12657\n",
      "Number of rows after:  12657\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'country' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['country'].str.contains(r'[A-Z]{2}')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the column 'domain_rank', which should be a number. Let's verify that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'domain_rank' collumn <-\n",
      "Number of rows before:  12657\n",
      "Type of 'domain_rank': float64\n",
      "Is it int64? False\n",
      "Number of rows after:  12657\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'domain_rank' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'domain_rank':\", fake['domain_rank'].dtypes)\n",
    "print(\"Is it int64?\", fake['domain_rank'].dtypes=='int64')\n",
    "\n",
    "#cast to int64\n",
    "fake['domain_rank'] = fake['domain_rank'].apply(np.int64)\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake=fake[fake['domain_rank']>=0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is \"thread_title\". This column follows the same rules as the 'title' column, it is very hard to define what is a correct title or not. The only verifiable condition is the same as the one used before for 'title'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'thread_title' collumn <-\n",
      "Number of rows before:  12657\n",
      "Number of rows after:  12657\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'thread_title' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['thread_title'].str.contains(r'^\\s*$')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next column is \"spam_score\". This is a very unique column as it only must have float values between 0 and 1. Let's verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'spam_score' collumn <-\n",
      "Number of rows before:  12657\n",
      "Type of 'spam_score': float64\n",
      "Is it float64? True\n",
      "Number of rows after:  12657\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'spam_score' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'spam_score':\", fake['spam_score'].dtypes)\n",
    "print(\"Is it float64?\", fake['spam_score'].dtypes == 'float64')\n",
    "\n",
    "#check if all values are greater or equal than 0 and lower or equal than 1\n",
    "fake = fake[fake['spam_score'] >= 0]\n",
    "fake = fake[fake['spam_score'] <= 1]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next column is \"replies_count\" which, as a counter, should be a positive integer or zero. Let's check those conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'replies_count' collumn <-\n",
      "Number of rows before:  12657\n",
      "Type of 'replies_count': int64\n",
      "Is it int64? True\n",
      "Number of rows after:  12657\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'replies_count' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'replies_count':\", fake['replies_count'].dtypes)\n",
    "print(\"Is it int64?\", fake['replies_count'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['replies_count'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same verification as \"replies_count\" will happen for the columns \"participants_count\", \"likes\", \"comments\" and \"shares\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'participants_count' collumn <-\n",
      "Number of rows before:  12657\n",
      "Type of 'participants_count': int64\n",
      "Is it int64? True\n",
      "Number of rows after:  12657\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'participants_count' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'participants_count':\", fake['participants_count'].dtypes)\n",
    "print(\"Is it int64?\", fake['participants_count'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['participants_count'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'likes' collumn <-\n",
      "Number of rows before:  12657\n",
      "Type of 'likes': int64\n",
      "Is it int64? True\n",
      "Number of rows after:  12657\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'likes' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'likes':\", fake['likes'].dtypes)\n",
    "print(\"Is it int64?\", fake['likes'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['likes'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'comments' collumn <-\n",
      "Number of rows before:  12657\n",
      "Type of 'comments': int64\n",
      "Is it int64? True\n",
      "Number of rows after:  12657\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'comments' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'comments':\", fake['comments'].dtypes)\n",
    "print(\"Is it int64?\", fake['comments'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['comments'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'shares' collumn <-\n",
      "Number of rows before:  12657\n",
      "Type of 'shares': int64\n",
      "Is it int64? True\n",
      "Number of rows after:  12657\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'shares' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'shares':\", fake['shares'].dtypes)\n",
    "print(\"Is it int64?\", fake['shares'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['shares'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least we have the \"type\" column, which contains a lowercase string classifying the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Cleaning 'type' collumn <-\n",
      "Number of rows before:  12657\n",
      "Number of rows after:  12657\n"
     ]
    }
   ],
   "source": [
    "print(\"-> Cleaning 'type' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['type'].str.contains(r'^[a-z]+$')]  # lowercase letters\n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['type'].str.contains(r'^\\s*$')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, since the data cleaning is finished, we can save the new clean data to a new csv file, to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.to_csv(\"fake_clean.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63025f8606b7d6daf5c9b2f011dca5c60db3c92c71df34b1c114cc7a6bb35c39"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
