{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Let's start by removing any data duplicates that add nothing to the dataset. We should compare the number of rows before and after removing the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before:  12999\n",
      "Number of rows after:  12999\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake.drop_duplicates()\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that there are no duplicate rows. Moving on to missing data.\n",
    "\n",
    "We should find all the rows with missing data and acknowledge every missing information column-wise. Therefore, we should see check for each column the missing information and see if we should either remove the collumn totally or find a viable substitution for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_rank           4223\n",
      "main_img_url          3643\n",
      "author                2424\n",
      "title                  680\n",
      "country                176\n",
      "text                    46\n",
      "thread_title            12\n",
      "shares                   0\n",
      "comments                 0\n",
      "likes                    0\n",
      "participants_count       0\n",
      "replies_count            0\n",
      "uuid                     0\n",
      "spam_score               0\n",
      "ord_in_thread            0\n",
      "site_url                 0\n",
      "crawled                  0\n",
      "language                 0\n",
      "published                0\n",
      "type                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(fake.isnull().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a descending order of the number of missing values per collumn. We will now analyse each column and study if it is worth \"fixing\" or substituting the missing values or just delete the column all together.\n",
    "\n",
    "As we can see, we have a column named \"main_img_url\" that doesn't provide any useful data for the study of this dataset. Because of that, we decided to remove it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.drop(['main_img_url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the column \"author\" there are two different cases that caught our attention: there are \"anonymous\" authors and just missing authors. Since both of these cases are comparable, because there is no info about the author in neither of them, we decided to make them all the same and add \"anonymous\" to the rows where the \"author\" info is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12999 entries, 0 to 12998\n",
      "Data columns (total 19 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   uuid                12999 non-null  object \n",
      " 1   ord_in_thread       12999 non-null  int64  \n",
      " 2   author              12999 non-null  object \n",
      " 3   published           12999 non-null  object \n",
      " 4   title               12319 non-null  object \n",
      " 5   text                12953 non-null  object \n",
      " 6   language            12999 non-null  object \n",
      " 7   crawled             12999 non-null  object \n",
      " 8   site_url            12999 non-null  object \n",
      " 9   country             12823 non-null  object \n",
      " 10  domain_rank         8776 non-null   float64\n",
      " 11  thread_title        12987 non-null  object \n",
      " 12  spam_score          12999 non-null  float64\n",
      " 13  replies_count       12999 non-null  int64  \n",
      " 14  participants_count  12999 non-null  int64  \n",
      " 15  likes               12999 non-null  int64  \n",
      " 16  comments            12999 non-null  int64  \n",
      " 17  shares              12999 non-null  int64  \n",
      " 18  type                12999 non-null  object \n",
      "dtypes: float64(2), int64(6), object(11)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "fake[\"author\"]=fake[\"author\"].fillna(\"Anonymous\")\n",
    "fake.to_csv(\"fake_2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
