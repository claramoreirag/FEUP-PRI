{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Fake News Analyzer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps\n",
    "\n",
    " Here we import all the libraries needed for the project and import the data from the CSV. All the data is in one csv named \"fake.csv\". This csv has got several rows and columns that will be evaluated in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> FAKE NEWS ANALYZER <-\n                                       uuid  ord_in_thread  \\\n0  6a175f46bcd24d39b3e962ad0f29936721db70db              0   \n1  2bdc29d12605ef9cf3f09f9875040a7113be5d5b              0   \n2  c70e149fdd53de5e61c29281100b9de0ed268bc3              0   \n3  7cf7c15731ac2a116dd7f629bd57ea468ed70284              0   \n4  0206b54719c7e241ffe0ad4315b808290dbe6c0f              0   \n\n                 author                      published  \\\n0     Barracuda Brigade  2016-10-26T21:41:00.000+03:00   \n1  reasoning with facts  2016-10-29T08:47:11.259+03:00   \n2     Barracuda Brigade  2016-10-31T01:41:49.479+02:00   \n3                Fed Up  2016-11-01T05:22:00.000+02:00   \n4                Fed Up  2016-11-01T21:56:00.000+02:00   \n\n                                               title  \\\n0  Muslims BUSTED: They Stole Millions In Gov’t B...   \n1  Re: Why Did Attorney General Loretta Lynch Ple...   \n2  BREAKING: Weiner Cooperating With FBI On Hilla...   \n3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n\n                                                text language  \\\n0  Print They should pay all the back all the mon...  english   \n1  Why Did Attorney General Loretta Lynch Plead T...  english   \n2  Red State : \\r\\nFox News Sunday reported this ...  english   \n3  Email Kayla Mueller was a prisoner and torture...  english   \n4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  english   \n\n                         crawled             site_url country  domain_rank  \\\n0  2016-10-27T01:49:27.168+03:00  100percentfedup.com      US      25689.0   \n1  2016-10-29T08:47:11.259+03:00  100percentfedup.com      US      25689.0   \n2  2016-10-31T01:41:49.479+02:00  100percentfedup.com      US      25689.0   \n3  2016-11-01T15:46:26.304+02:00  100percentfedup.com      US      25689.0   \n4  2016-11-01T23:59:42.266+02:00  100percentfedup.com      US      25689.0   \n\n                                        thread_title  spam_score  \\\n0  Muslims BUSTED: They Stole Millions In Gov’t B...       0.000   \n1  Re: Why Did Attorney General Loretta Lynch Ple...       0.000   \n2  BREAKING: Weiner Cooperating With FBI On Hilla...       0.000   \n3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...       0.068   \n4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...       0.865   \n\n                                        main_img_url  replies_count  \\\n0  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n1  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n2  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n3  http://100percentfedup.com/wp-content/uploads/...              0   \n4  http://100percentfedup.com/wp-content/uploads/...              0   \n\n   participants_count  likes  comments  shares  type  \n0                   1      0         0       0  bias  \n1                   1      0         0       0  bias  \n2                   1      0         0       0  bias  \n3                   0      0         0       0  bias  \n4                   0      0         0       0  bias  \n"
    }
   ],
   "source": [
    "#imports and reading dataset\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import nltk\n",
    "import numpy as np\n",
    "import csv \n",
    "import json \n",
    "print(\"-> FAKE NEWS ANALYZER <-\")\n",
    "\n",
    "fake = pd.read_csv(\"./archive/fake.csv\", na_values=[\"\"])\n",
    "print(fake.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see it is a pretty big dataset. Let's talk numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of rows:  12999\nNumber of columns:  20\n"
    }
   ],
   "source": [
    "print(\"Number of rows: \", len(fake.index))\n",
    "\n",
    "print(\"Number of columns: \", len(fake.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12999 rows and 20 different columns. This constitutes a big sample of fake news to analyse. Let's now see which type of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Printing collumn's data types <-\nuuid                   object\nord_in_thread           int64\nauthor                 object\npublished              object\ntitle                  object\ntext                   object\nlanguage               object\ncrawled                object\nsite_url               object\ncountry                object\ndomain_rank           float64\nthread_title           object\nspam_score            float64\nmain_img_url           object\nreplies_count           int64\nparticipants_count      int64\nlikes                   int64\ncomments                int64\nshares                  int64\ntype                   object\ndtype: object\n"
    }
   ],
   "source": [
    "print(\"-> Printing collumn's data types <-\")\n",
    "\n",
    "print(fake.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Duplicates\n",
    "\n",
    "Let's start by removing any data duplicates that add nothing to the dataset. We should compare the number of rows before and after removing the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> DATA CLEANING <-\n-> Dropping duplicate rows <-\nNumber of rows before:  12999\nNumber of rows after:  12999\n"
    }
   ],
   "source": [
    "print(\"-> DATA CLEANING <-\")\n",
    "\n",
    "print(\"-> Dropping duplicate rows <-\")\n",
    "\n",
    "#cleaning duplicates\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake.drop_duplicates()\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that there are no duplicate rows. Moving on to missing data.\n",
    "\n",
    "## Missing data\n",
    "\n",
    "We should find all the rows with missing data and acknowledge every missing information column-wise. Therefore, we should see check for each column the missing information and see if we should either remove the collumn totally or find a viable substitution for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Showing null values per collumn - before cleaning <-\ndomain_rank           4223\nmain_img_url          3643\nauthor                2424\ntitle                  680\ncountry                176\ntext                    46\nthread_title            12\ncrawled                  0\nord_in_thread            0\npublished                0\nlanguage                 0\ntype                     0\nsite_url                 0\nshares                   0\nspam_score               0\nreplies_count            0\nparticipants_count       0\nlikes                    0\ncomments                 0\nuuid                     0\ndtype: int64\n"
    }
   ],
   "source": [
    "print(\"-> Showing null values per collumn - before cleaning <-\")\n",
    "\n",
    "print(fake.isnull().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a descending order of the number of missing values per collumn. We will now analyse each column and study if it is worth \"fixing\" or substituting the missing values or just delete the column all together.\n",
    "\n",
    "As we can see, we have a column named \"main_img_url\" that doesn't provide any useful data for the study of this dataset. Because of that, we decided to remove it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Dropping 'main_img_url' collumn <-\n"
    }
   ],
   "source": [
    "print(\"-> Dropping 'main_img_url' collumn <-\")\n",
    "\n",
    "fake.drop(['main_img_url'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the column \"author\" there are two different cases that caught our attention: there are \"anonymous\" authors and just missing authors. Since both of these cases are comparable, because there is no info about the author in neither of them, we decided to make them all the same and add \"anonymous\" to the rows where the \"author\" info is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Fixing null values in 'author' <-\n"
    }
   ],
   "source": [
    "print(\"-> Fixing null values in 'author' <-\")\n",
    "\n",
    "fake[\"author\"]=fake[\"author\"].fillna(\"Anonymous\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can also see, there are 12 rows without a thread title. This doesn't allow us group up the fake news into threads because we don't know where they belong. Since it is such a small ammount of news (12) we decided that they should removed as they don't represent a big sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Fixing null values in 'thread_title' <-\nNumber of rows before:  12999\nNumber of rows after:  12987\n"
    }
   ],
   "source": [
    "print(\"-> Fixing null values in 'thread_title' <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['thread_title'].notna()]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to evaluate and perform text search based on the text of the news, news with no text become irrelevant to this dataset. Therefore, we are going to remove those rows as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Fixing null values in 'text' <-\nNumber of rows before:  12987\nNumber of rows after:  12941\n"
    }
   ],
   "source": [
    "print(\"-> Fixing null values in 'text' <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['text'].notna()]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are news with no title which we have found to be in the same thread where only the news in first place (order_in_thread=0) has got a title. Let's verify if all the existing titles are the same as the thread_title associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of unique titles: 11653\nNumber of unique thread_titles: 11742 \n\nDifferent thread_title: Did Google Kill Julian Assange?\nDifferent thread_title: Prominent Democrat Connected To Clintons Donated $675,000 To Campaign Of Deputy FBI Director's Wife\nDifferent thread_title: Is it coming into clearer focus for Americans, or not?\nDifferent thread_title: Weiner Sings Like a Bird as Dickileak Provides Exit as Huma Sequestered by FBI at MOUNT WEATHER - After Huma's 65,000 Emails Found...No Longer Together\nDifferent thread_title: Installing a President by Force: Hillary Clinton and Our Moribund Democracy\nDifferent thread_title: Clinton's Policies Look Like a Death Sentence for Americans - American Herald Tribune\nDifferent thread_title: is bed the EU made, now it must lie in it -\nDifferent thread_title: Justin Trudeau Doesn't Tell the Truth to Canadians about Syria\nDifferent thread_title: 10,000 children are missing and the world just looks away\nDifferent thread_title: The Orlando Shooting and our reptilian minds\nDifferent thread_title: Anonymous: Bill Clinton Pedophile Sex Tape Filmed By Jeffrey Epstein!\nDifferent thread_title: Fears Grow Julian Assange Was Extradited On ‘Guantanamo Express’\nDifferent thread_title: Promoting Peace: A Large Number of Israeli and Palestinian Women March Together\nDifferent thread_title: Surgeons Admit Mammography is Outdated, Harmful at Best\nDifferent thread_title: New Report Reveals Modern Diets are Making People Sicker Across the World\nDifferent thread_title: Cheap Wind-Powered Device Produces 11 Gallons of Clean Drinking Water Per Day from the Air\nDifferent thread_title: Everything You Knew About the Paleo Diet is Wrong. Here’s the Proof\nDifferent thread_title: Barack Obama ‘Reveals’: How NASA Will Send Humans To Mars By The 2030s\nDifferent thread_title: Wisconsin Supreme Court Guts Fourth Amendment in Unprecedented Decision\nDifferent thread_title: Ecuador Admits They Silenced Assange Because Clinton Leaks Were ‘Interfering’ With US Election\nDifferent thread_title: The History of the House of Rothschild (1743-2006)\nDifferent thread_title: Crash Victim Calls for Help - Gets Arrested and Sexually Assaulted Instead\nDifferent thread_title: Pennsylvania Pipelines Bursts, Leaks 55,000 Gallons Of Gas Into One Of US’ Most Endangered Rivers\nDifferent thread_title: Crash Victim Calls Police for Help - Gets Arrested and Sexually Assaulted Instead\nDifferent thread_title: Join Us Worldwide for the 2016 Million Mask March!\nDifferent thread_title: 7 Insidious Ways the Elite Are Making Americans Dumber (Video) | Power Elite\nDifferent thread_title: Clinton & Soros Bribing the Electoral College Voters-Massive Voter Fraud in AZ | Conspiracy Theories\nDifferent thread_title: Analysis: Election Outcome Scenarios Reveal 95% Chance of Widespread Post-Election Violence… Streets of America to Run Red With Blood | War and Conflict\nDifferent thread_title: This Is Definitely Pissing a Lot of Hillary People Off! | Alternative\nDifferent thread_title: X22Report Shadow Banking Is Back, and This Time There Is No Hope - Episode 1110a | Politics\nDifferent thread_title: Not Without the Consent of the Governed | Police State\nDifferent thread_title: The Mothership Arrives --- But It's Not What You Think! | Alternative\nDifferent thread_title: Greg Hunter, \"USA Actually Bankrupt Now\" | Opinion - Liberal\nDifferent thread_title: Paul Craig Roberts: ‘By Cooperating with Washington on Syria Russia Walked Into a Trap’ | Alternative\nDifferent thread_title: Breaking! Wikileaks to Take Down Top Clinton Strategist (Video) | Prophecy\nDifferent thread_title: Red Warning – Nuclear Attack Russian Surprise for US Military – Surrounded USA NATO Military Drills on Russian Border – Putin Issues a Ultimatum to the United States | Self-Sufficiency\nDifferent thread_title: WATCH: Muslim Worshipers Mistake Synagogue For Mosque\nDifferent thread_title: California Politicians Demand National Guard Forgive Enlistment Bonuses\nDifferent thread_title: 2 Somali Immigrants Guilty of Aiding Foreign Terrorists\nDifferent thread_title: Duterte: Philippines Will Not Be a 'Dog Barking for Crumbs' from U.S.\nDifferent thread_title: Report: Islamic State Incinerates Jihadis in Burning Oil for Fleeing Mosul Battle\nDifferent thread_title: Islamic State Terrorists 'Shave Beards', Prepare to Flee Mosul\nDifferent thread_title: UNESCO Approves Motion Denying Jewish, Christian Links to Temple Mount\nDifferent thread_title: Iran Unveils Attack 'Suicide Drone'\nDifferent thread_title: Pentagon Warns Chinese Computer Parts May Compromise Cyber-Security\nDifferent thread_title: U.S. Pledges Another $800 Million to Afghanistan\nDifferent thread_title: Russia TV Promoting Nuclear War Drills as U.S. Deploys Troops to Norway\nDifferent thread_title: Women Love to Objectify Themselves: Here’s Why\nDifferent thread_title: “Pedophiles are the New Jews”\nDifferent thread_title: Ellen Degeneres: Quits Talk Show to Save Portia De Rossi Marriage?\nDifferent thread_title: Tumbler \"Come to Edge\" Underground Examinations Music Review\nDifferent thread_title: Nelson Mandela’s Top Five Contributions to Humanity\nDifferent thread_title: Garth Brooks Sings About Divorce from Trisha in 'The Call'\nDifferent thread_title: NASA Confirmed Alien Invasion?\nDifferent thread_title: NASA Weighs in on Apocalyptic Sounds From Heaven Heard During Month of Pentecost and Shavuot [Videos]\nDifferent thread_title: NASA Confirms -Super Human Abilities Gained\nDifferent thread_title: Is Mel Gibson on Steroids?\nDifferent thread_title: Christianity and Liberalism Cannot Exist Together\nDifferent thread_title: The Double Standards of American Politics (What Do I Tell My Daughter?)\nDifferent thread_title: Could Real Life 'Purge' Threat Happen This Weekend?\nDifferent thread_title: HPV: The Quiet Virus That Causes Very Loud Problems\nDifferent thread_title: KKK and Racist Groups Have Killed More People Than Islamist Terrorists in the U.S. Since 9/11\nDifferent thread_title: Television: Highest-Paid Actors and Actresses to Date\nDifferent thread_title: Celebrities Why Do They Have so Much Influence?\nDifferent thread_title: Mel Gibson Starring in 'Blood Father'\nDifferent thread_title: Battlestar Galactica Gets Another Remake\nDifferent thread_title: WHERES THERE A CRASH? WORLD ECONOMY COLLAPSING!!! - HEADLINES OCTOBER 2016\nDifferent thread_title: Time To Panic About Deutsche Bank & Credit Suisse! - Probably The Next Lehman\nDifferent thread_title: Important Vid Fukushima I Nuclear Power Plant\nDifferent thread_title: HACKING ATTACKS\nDifferent thread_title: WINDOWS 10\nDifferent thread_title: GOOGLE - ALPHABET\nDifferent thread_title: APPLE\nDifferent thread_title: Communism, fascism, nazism, Marxism ALL created by the Vatican not Jews\nDifferent thread_title: Hitler hated the Slavs more than a Jews ?\nDifferent thread_title: How Do We Get to a Conversation in This Country About Climate?\nDifferent thread_title: Obamacare Premiums Are Set to Increase Steeply Next Year\nDifferent thread_title: One of the Biggest Media Mergers Ever May Be on the Horizon\nDifferent thread_title: Five Terrifying Things From Trump's Blueprint for His First 100 Days if Elected\nDifferent thread_title: Unforeseen Consequences: The Death of Trees\nDifferent thread_title: Donald Trump Could Have Manufactured His Products in the US; He Chose China\nDifferent thread_title: \"We've Never Seen the Government Stand Up to the Fossil Fuel Industry\": Tim DeChristopher on Our Climate Future\nDifferent thread_title: Donald Trump's Star On Hollywood Walk Of Fame Vandalized\nDifferent thread_title: Judge Files Criminal Charge Against Sheriff Joe Arpaio For Enforcing Federal Immigration Laws\nDifferent thread_title: Independent Voters Push Trump To The Front In Florida And Ohio\nDifferent thread_title: O'Reilly Grills Washington Post Columnist Who Says He Is Not A 'Real' News Person\nDifferent thread_title: Same-Sex Marriage And The Politics Of Aggression\nDifferent thread_title: Hannity Proposes A Sendoff For Obama In The Event Of A Trump Presidency\nDifferent thread_title: Caught On Tape: ISIS Destroys Abrams Tank With Anti-Tank Missile\nDifferent thread_title: Tesla Earnings Smash Expectations After Dramatic Change In Reporting Methodology\n"
    }
   ],
   "source": [
    "# get unique values from each column\n",
    "titles = fake['title'].unique()\n",
    "thread_titles = fake['thread_title'].unique()\n",
    "\n",
    "#check if size is the same\n",
    "print(\"Number of unique titles:\", len(titles))\n",
    "print(\"Number of unique thread_titles:\", len(thread_titles),\"\\n\")\n",
    "\n",
    "# convert every value to string\n",
    "for i in range(0, len(titles)):\n",
    "    titles[i]=str(titles[i])\n",
    "\n",
    "for i in range(0, len(thread_titles)):\n",
    "    thread_titles[i] = str(thread_titles[i])\n",
    "\n",
    "#sort arrays\n",
    "np.sort(titles)\n",
    "np.sort(thread_titles)\n",
    "\n",
    "\n",
    "# print every different title - compare to bigger list: thread_titles\n",
    "for element in thread_titles:\n",
    "    if(element not in titles):\n",
    "        print(\"Different thread_title:\", element)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are different titles and thread_titles, making it impossible for us to eliminate one of those columns. We can, however, substitute the data and eliminate rows which don't have any of these informations, as it is impossible for us to track the news associated to any theme or key word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of rows with no title and no thread_title: 0\n"
    }
   ],
   "source": [
    "print(\"Number of rows with no title and no thread_title:\", len(fake.loc[fake['title'].isna() & fake['thread_title'].isna()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no rows with neither of the values in the referred columns, we should turn our attention to how to correct the missing data in these columns. For every row with no title, we will substitute the value with the thread_title instead, since the thread_title will be in some way connected to the theme of the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Fixing null values in 'title' <-\n"
    }
   ],
   "source": [
    "print(\"-> Fixing null values in 'title' <-\")\n",
    "\n",
    "fake.title.fillna(fake.thread_title, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, there are news with no 'country' value associated, more specifically, 176 news. To make it easier to analyze this data and not have missing values, we will drop these rows, since we can't track the origin of the news and it is a rather small ammount of data that is discarded (176 rows), given the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Fixing null values in 'country' <-\nNumber of rows before:  12941\nNumber of rows after:  12765\n"
    }
   ],
   "source": [
    "print(\"-> Fixing null values in 'country' <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['country'].notna()]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As domain rank is a metric, going from 0 to 100, the later being the strongest, evaluated based on many factors dependent on user searches and the authority of a certain domain, if a domain doesn't have an available ranking, we can suppose it is 0 since it hasn't been evaluated yet. That can be due to lack of information or visits to that domain. That being said, we will substitute every missing value in the column \"domain_rank\" by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Fixing null values in 'domain_rank' <-\n"
    }
   ],
   "source": [
    "print(\"-> Fixing null values in 'domain_rank' <-\")\n",
    "\n",
    "fake[\"domain_rank\"] = fake[\"domain_rank\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a new look at the null values per column and see the final dataset, with no missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Showing null values per collumn - after cleaning <-\ntype                  0\nsite_url              0\nord_in_thread         0\nauthor                0\npublished             0\ntitle                 0\ntext                  0\nlanguage              0\ncrawled               0\ncountry               0\nshares                0\ndomain_rank           0\nthread_title          0\nspam_score            0\nreplies_count         0\nparticipants_count    0\nlikes                 0\ncomments              0\nuuid                  0\ndtype: int64\n"
    }
   ],
   "source": [
    "print(\"-> Showing null values per collumn - after cleaning <-\")\n",
    "\n",
    "print(fake.isnull().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we have observed that uuid column doesn't really add anything of value to our dataset, therefore, we will drop the column all together before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Dropping 'uuid' collumn <-\n"
    }
   ],
   "source": [
    "print(\"-> Dropping 'uuid' collumn <-\")\n",
    "\n",
    "fake.drop(['uuid'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broken Data & Data Types\n",
    "\n",
    "As in all datasets, there are rows with broken data, as in, data that was unsuccessfully crawled and filled in rows with broken values. Let's find these rows in our dataset and correct or drop them.\n",
    "\n",
    "The first case we noticed were some rows filled with 0 and 1, namely in the title and text columns. Let's locate them and print them, to analize the data and see if it should be dropped. If we can't find any and since we already cleaned some of the data, it means they were already dropped in the steps before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning crawling errors <-\nEmpty DataFrame\nColumns: [ord_in_thread, author, published, title, text, language, crawled, site_url, country, domain_rank, thread_title, spam_score, replies_count, participants_count, likes, comments, shares, type]\nIndex: []\nEmpty DataFrame\nColumns: [ord_in_thread, author, published, title, text, language, crawled, site_url, country, domain_rank, thread_title, spam_score, replies_count, participants_count, likes, comments, shares, type]\nIndex: []\nEmpty DataFrame\nColumns: [ord_in_thread, author, published, title, text, language, crawled, site_url, country, domain_rank, thread_title, spam_score, replies_count, participants_count, likes, comments, shares, type]\nIndex: []\nEmpty DataFrame\nColumns: [ord_in_thread, author, published, title, text, language, crawled, site_url, country, domain_rank, thread_title, spam_score, replies_count, participants_count, likes, comments, shares, type]\nIndex: []\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning crawling errors <-\")\n",
    "\n",
    "print(fake.loc[fake['title'] == \"0\"])\n",
    "print(fake.loc[fake['title'] == \"1\"])\n",
    "\n",
    "print(fake.loc[fake['text'] == \"0\"])\n",
    "print(fake.loc[fake['text'] == \"1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional verification that the above statement was correct, we opened the dataset in excel and verified that every case of broken data was indeed gone from the dataset at this point of the cleaning.\n",
    "\n",
    "The other big case for broken data that we found were some news that weren't correctly crawled and just extended the text of the news throughout several columns. There are several ways of correcting this issue, but in order to get all the cases and clean the data the most, we will start by verifying column by column if the data types and formats are correct, dropping the columns in which that isn't verified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with 'ord_in_thread' column. It must be a number, therefore let's eliminate every row that doesn't just contain a number. If there is a row with alpha characters in it, the type of the column won't be int64 so let's first verify the type and eliminate non numeric rows if it isn't 'int64'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'ord_in_thread' collumn <-\nNumber of rows before:  12765\nType of 'ord_in_thread': int64\nIs it int64? True\nNumber of rows after:  12765\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'ord_in_thread' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'ord_in_thread':\", fake['ord_in_thread'].dtypes)\n",
    "print(\"Is it int64?\", fake['ord_in_thread'].dtypes == 'int64')\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it returns 'int64' we are sure that the whole column only contains numeric values. Moving on to 'author', there are 2 errors to fix: wrong datatypes and the \"-NO AUTHOR-\" rows. As it is very hard to verify the correctness of the author name, as it is very hard to define \"correctness\" in these cases, and not exclude valid names with numeric digits using the usual rules for identifying names (since authors can have numbers in their username), we will move on to correcting the \"-NO AUTHOR-\" cases, where it should be replaced by \"Anonymous\", as we did before with null values. We will also remove rows with just whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'author' collumn <-\nNumber of rows before:  12765\nNumber of rows after:  12765\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'author' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake[\"author\"] = fake[\"author\"].replace(\"-NO AUTHOR-\",\"Anonymous\")\n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['author'].str.contains(r'^\\s*$')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we have the column \"published\", that should only contain dates. We can verify this through a regex expression that verifies the format of the dates, as well as the datatypes present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'published' collumn <-\nNumber of rows before:  12765\nNumber of rows after:  12765\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'published' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake=fake[fake['published'].str.contains('\\d{4}-\\d{2}-\\d{2}')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next column to verify is the column \"title\". We have cleaned this column before so some work was already done. We should just verify that the titles aren't mere strings of whitespaces, as it is extremely difficult to define a \"correct\" title or text in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'title' collumn <-\nNumber of rows before:  12765\nNumber of rows after:  12765\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'title' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['title'].str.contains(r'^\\s*$')]\n",
    "\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the \"text\" column, we must do the same verification as in the \"title\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'text' collumn <-\nNumber of rows before:  12765\nNumber of rows after:  12657\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'text' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[~fake['text'].str.contains(r'^\\s*$')]  #clean string rows with just whitespace\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we have the \"language\" column. This column must only contain one word and no digits, so the funtion isalpha() will do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'language' collumn <-\nNumber of rows before:  12657\nNumber of rows after:  12650\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'language' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake['language'] = fake['language'].apply(str)\n",
    "fake = fake[fake.language.str.isalpha()] \n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['language'].str.contains(r'^\\s*$')]\n",
    "fake = fake[fake.language != \"ignore\"]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the column \"crawled\", which should only contain the date in which the data was crawled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'crawled' collumn <-\nNumber of rows before:  12650\nNumber of rows after:  12650\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'crawled' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['crawled'].str.contains(r'\\d{4}-\\d{2}-\\d{2}')]  # dates\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following column is \"site_url\". Every site follows the same structure of domain.xxx, so we will apply a regex expression to filter unwanted formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'site_url' collumn <-\nNumber of rows before:  12650\nNumber of rows after:  12650\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'site_url' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['site_url'].str.contains(r'^(www\\.)?(.*)?\\/?(.)*')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the 'country' column, the countries are mentioned as a 2 uppercase letter code. Let's confirm every row follows that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'country' collumn <-\nNumber of rows before:  12650\nNumber of rows after:  12650\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'country' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['country'].str.contains(r'[A-Z]{2}')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the column 'domain_rank', which should be a number. Let's verify that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'domain_rank' collumn <-\nNumber of rows before:  12650\nType of 'domain_rank': float64\nIs it int64? False\nNumber of rows after:  12650\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'domain_rank' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'domain_rank':\", fake['domain_rank'].dtypes)\n",
    "print(\"Is it int64?\", fake['domain_rank'].dtypes=='int64')\n",
    "\n",
    "#cast to int64\n",
    "fake['domain_rank'] = fake['domain_rank'].apply(np.int64)\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake=fake[fake['domain_rank']>=0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is \"thread_title\". This column follows the same rules as the 'title' column, it is very hard to define what is a correct title or not. The only verifiable condition is the same as the one used before for 'title'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'thread_title' collumn <-\nNumber of rows before:  12650\nNumber of rows after:  12650\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'thread_title' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['thread_title'].str.contains(r'^\\s*$')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next column is \"spam_score\". This is a very unique column as it only must have float values between 0 and 1. Let's verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'spam_score' collumn <-\nNumber of rows before:  12650\nType of 'spam_score': float64\nIs it float64? True\nNumber of rows after:  12650\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'spam_score' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'spam_score':\", fake['spam_score'].dtypes)\n",
    "print(\"Is it float64?\", fake['spam_score'].dtypes == 'float64')\n",
    "\n",
    "#check if all values are greater or equal than 0 and lower or equal than 1\n",
    "fake = fake[fake['spam_score'] >= 0]\n",
    "fake = fake[fake['spam_score'] <= 1]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next column is \"replies_count\" which, as a counter, should be a positive integer or zero. Let's check those conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'replies_count' collumn <-\nNumber of rows before:  12650\nType of 'replies_count': int64\nIs it int64? True\nNumber of rows after:  12650\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'replies_count' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'replies_count':\", fake['replies_count'].dtypes)\n",
    "print(\"Is it int64?\", fake['replies_count'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['replies_count'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same verification as \"replies_count\" will happen for the columns \"participants_count\", \"likes\", \"comments\" and \"shares\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'participants_count' collumn <-\nNumber of rows before:  12650\nType of 'participants_count': int64\nIs it int64? True\nNumber of rows after:  12650\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'participants_count' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'participants_count':\", fake['participants_count'].dtypes)\n",
    "print(\"Is it int64?\", fake['participants_count'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['participants_count'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'likes' collumn <-\nNumber of rows before:  12650\nType of 'likes': int64\nIs it int64? True\nNumber of rows after:  12650\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'likes' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'likes':\", fake['likes'].dtypes)\n",
    "print(\"Is it int64?\", fake['likes'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['likes'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'comments' collumn <-\nNumber of rows before:  12650\nType of 'comments': int64\nIs it int64? True\nNumber of rows after:  12650\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'comments' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'comments':\", fake['comments'].dtypes)\n",
    "print(\"Is it int64?\", fake['comments'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['comments'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'shares' collumn <-\nNumber of rows before:  12650\nType of 'shares': int64\nIs it int64? True\nNumber of rows after:  12650\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'shares' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'shares':\", fake['shares'].dtypes)\n",
    "print(\"Is it int64?\", fake['shares'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['shares'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least we have the \"type\" column, which contains a lowercase string classifying the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Cleaning 'type' collumn <-\nNumber of rows before:  12650\nNumber of rows after:  12650\n"
    }
   ],
   "source": [
    "print(\"-> Cleaning 'type' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['type'].str.contains(r'^[a-z]+$')]  # lowercase letters\n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['type'].str.contains(r'^\\s*$')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, since the data cleaning is finished, we can save the new clean data to a new csv file, to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.to_csv(\"fake_clean.csv\", index=True)\n",
    "fake.to_json(\"fake_clean.json\", orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def csv_to_json(csvFilePath, jsonFilePath):\n",
    "#     jsonArray = []\n",
    "      \n",
    "#     #read csv file\n",
    "#     with open(csvFilePath, encoding='utf-8') as csvf: \n",
    "#         #load csv file data using csv library's dictionary reader\n",
    "#         csvReader = csv.DictReader(csvf) \n",
    "\n",
    "#         #convert each csv row into python dict\n",
    "#         for row in csvReader: \n",
    "#             #add this python dict to json array\n",
    "#             jsonArray.append(row)\n",
    "  \n",
    "#     #convert python jsonArray to JSON String and write to file\n",
    "#     with open(jsonFilePath, 'w', encoding='utf-8') as jsonf: \n",
    "#         jsonString = json.dumps(jsonArray, indent=4)\n",
    "#         jsonf.write(jsonString)\n",
    "          \n",
    "# csv_file = r'fake_clean.csv'\n",
    "# json_file = r'fake_clean.json'\n",
    "# csv_to_json(csv_file, json_file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63025f8606b7d6daf5c9b2f011dca5c60db3c92c71df34b1c114cc7a6bb35c39"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9-final"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}