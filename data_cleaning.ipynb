{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Fake News Analyzer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps\n",
    "\n",
    " Here we import all the libraries needed for the project and import the data from the CSV. All the data is in one csv named \"fake.csv\". This csv has got several rows and columns that will be evaluated in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and reading dataset\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "print(\"-> FAKE NEWS ANALYZER <-\")\n",
    "\n",
    "fake = pd.read_csv(\"./archive/fake.csv\", na_values=[\"\"])\n",
    "print(fake.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see it is a pretty big dataset. Let's talk numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows: \", len(fake.index))\n",
    "\n",
    "print(\"Number of columns: \", len(fake.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12999 rows and 20 different columns. This constitutes a big sample of fake news to analyse. Let's now see which type of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Printing collumn's data types <-\")\n",
    "\n",
    "print(fake.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Duplicates\n",
    "\n",
    "Let's start by removing any data duplicates that add nothing to the dataset. We should compare the number of rows before and after removing the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> DATA CLEANING <-\")\n",
    "\n",
    "print(\"-> Dropping duplicate rows <-\")\n",
    "\n",
    "#cleaning duplicates\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake.drop_duplicates()\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that there are no duplicate rows. Moving on to missing data.\n",
    "\n",
    "## Missing data\n",
    "\n",
    "We should find all the rows with missing data and acknowledge every missing information column-wise. Therefore, we should see check for each column the missing information and see if we should either remove the collumn totally or find a viable substitution for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Showing null values per collumn - before cleaning <-\")\n",
    "\n",
    "print(fake.isnull().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a descending order of the number of missing values per collumn. We will now analyse each column and study if it is worth \"fixing\" or substituting the missing values or just delete the column all together.\n",
    "\n",
    "As we can see, we have a column named \"main_img_url\" that doesn't provide any useful data for the study of this dataset. Because of that, we decided to remove it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Dropping 'main_img_url' collumn <-\")\n",
    "\n",
    "fake.drop(['main_img_url'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the column \"author\" there are two different cases that caught our attention: there are \"anonymous\" authors and just missing authors. Since both of these cases are comparable, because there is no info about the author in neither of them, we decided to make them all the same and add \"anonymous\" to the rows where the \"author\" info is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Fixing null values in 'author' <-\")\n",
    "\n",
    "fake[\"author\"]=fake[\"author\"].fillna(\"Anonymous\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can also see, there are 12 rows without a thread title. This doesn't allow us group up the fake news into threads because we don't know where they belong. Since it is such a small ammount of news (12) we decided that they should removed as they don't represent a big sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Fixing null values in 'thread_title' <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['thread_title'].notna()]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to evaluate and perform text search based on the text of the news, news with no text become irrelevant to this dataset. Therefore, we are going to remove those rows as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Fixing null values in 'text' <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['text'].notna()]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are news with no title which we have found to be in the same thread where only the news in first place (order_in_thread=0) has got a title. Let's verify if all the existing titles are the same as the thread_title associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values from each column\n",
    "titles = fake['title'].unique()\n",
    "thread_titles = fake['thread_title'].unique()\n",
    "\n",
    "#check if size is the same\n",
    "print(\"Number of unique titles:\", len(titles))\n",
    "print(\"Number of unique thread_titles:\", len(thread_titles),\"\\n\")\n",
    "\n",
    "# convert every value to string\n",
    "for i in range(0, len(titles)):\n",
    "    titles[i]=str(titles[i])\n",
    "\n",
    "for i in range(0, len(thread_titles)):\n",
    "    thread_titles[i] = str(thread_titles[i])\n",
    "\n",
    "#sort arrays\n",
    "np.sort(titles)\n",
    "np.sort(thread_titles)\n",
    "\n",
    "\n",
    "# print every different title - compare to bigger list: thread_titles\n",
    "for element in thread_titles:\n",
    "    if(element not in titles):\n",
    "        print(\"Different thread_title:\", element)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are different titles and thread_titles, making it impossible for us to eliminate one of those columns. We can, however, substitute the data and eliminate rows which don't have any of these informations, as it is impossible for us to track the news associated to any theme or key word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows with no title and no thread_title:\", len(fake.loc[fake['title'].isna() & fake['thread_title'].isna()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no rows with neither of the values in the referred columns, we should turn our attention to how to correct the missing data in these columns. For every row with no title, we will substitute the value with the thread_title instead, since the thread_title will be in some way connected to the theme of the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Fixing null values in 'title' <-\")\n",
    "\n",
    "fake.title.fillna(fake.thread_title, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, there are news with no 'country' value associated, more specifically, 176 news. To make it easier to analyze this data and not have missing values, we will drop these rows, since we can't track the origin of the news and it is a rather small ammount of data that is discarded (176 rows), given the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Fixing null values in 'country' <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['country'].notna()]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As domain rank is a metric, going from 0 to 100, the later being the strongest, evaluated based on many factors dependent on user searches and the authority of a certain domain, if a domain doesn't have an available ranking, we can suppose it is 0 since it hasn't been evaluated yet. That can be due to lack of information or visits to that domain. That being said, we will substitute every missing value in the column \"domain_rank\" by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Fixing null values in 'domain_rank' <-\")\n",
    "\n",
    "fake[\"domain_rank\"] = fake[\"domain_rank\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a new look at the null values per column and see the final dataset, with no missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Showing null values per collumn - after cleaning <-\")\n",
    "\n",
    "print(fake.isnull().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we have observed that uuid column doesn't really add anything of value to our dataset, therefore, we will drop the column all together before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Dropping 'uuid' collumn <-\")\n",
    "\n",
    "fake.drop(['uuid'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broken Data & Data Types\n",
    "\n",
    "As in all datasets, there are rows with broken data, as in, data that was unsuccessfully crawled and filled in rows with broken values. Let's find these rows in our dataset and correct or drop them.\n",
    "\n",
    "The first case we noticed were some rows filled with 0 and 1, namely in the title and text columns. Let's locate them and print them, to analize the data and see if it should be dropped. If we can't find any and since we already cleaned some of the data, it means they were already dropped in the steps before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning crawling errors <-\")\n",
    "\n",
    "print(fake.loc[fake['title'] == \"0\"])\n",
    "print(fake.loc[fake['title'] == \"1\"])\n",
    "\n",
    "print(fake.loc[fake['text'] == \"0\"])\n",
    "print(fake.loc[fake['text'] == \"1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional verification that the above statement was correct, we opened the dataset in excel and verified that every case of broken data was indeed gone from the dataset at this point of the cleaning.\n",
    "\n",
    "The other big case for broken data that we found were some news that weren't correctly crawled and just extended the text of the news throughout several columns. There are several ways of correcting this issue, but in order to get all the cases and clean the data the most, we will start by verifying column by column if the data types and formats are correct, dropping the columns in which that isn't verified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with 'ord_in_thread' column. It must be a number, therefore let's eliminate every row that doesn't just contain a number. If there is a row with alpha characters in it, the type of the column won't be int64 so let's first verify the type and eliminate non numeric rows if it isn't 'int64'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'ord_in_thread' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'ord_in_thread':\", fake['ord_in_thread'].dtypes)\n",
    "print(\"Is it int64?\", fake['ord_in_thread'].dtypes == 'int64')\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it returns 'int64' we are sure that the whole column only contains numeric values. Moving on to 'author', there are 2 errors to fix: wrong datatypes and the \"-NO AUTHOR-\" rows. As it is very hard to verify the correctness of the author name, as it is very hard to define \"correctness\" in these cases, and not exclude valid names with numeric digits using the usual rules for identifying names (since authors can have numbers in their username), we will move on to correcting the \"-NO AUTHOR-\" cases, where it should be replaced by \"Anonymous\", as we did before with null values. We will also remove rows with just whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'author' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake[\"author\"] = fake[\"author\"].replace(\"-NO AUTHOR-\",\"Anonymous\")\n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['author'].str.contains(r'^\\s*$')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we have the column \"published\", that should only contain dates. We can verify this through a regex expression that verifies the format of the dates, as well as the datatypes present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'published' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake=fake[fake['published'].str.contains('\\d{4}-\\d{2}-\\d{2}')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next column to verify is the column \"title\". We have cleaned this column before so some work was already done. We should just verify that the titles aren't mere strings of whitespaces, as it is extremely difficult to define a \"correct\" title or text in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'title' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['title'].str.contains(r'^\\s*$')]\n",
    "\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the \"text\" column, we must do the same verification as in the \"title\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'text' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[~fake['text'].str.contains(r'^\\s*$')]  #clean string rows with just whitespace\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we have the \"language\" column. This column must only contain one word and no digits, so the funtion isalpha() will do the job. There are also some rows, 7 to be exact, which have the value 'ignore', which is not a language. These will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'language' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake['language'] = fake['language'].apply(str)\n",
    "fake = fake[fake.language.str.isalpha()] \n",
    "\n",
    "# clean string rows with just whitespace or 'ignore'\n",
    "fake = fake[~fake['language'].str.contains(r'^\\s*$')]\n",
    "fake = fake[fake.language != \"ignore\"]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the column \"crawled\", which should only contain the date in which the data was crawled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'crawled' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['crawled'].str.contains(r'\\d{4}-\\d{2}-\\d{2}')]  # dates\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following column is \"site_url\". Every site follows the same structure of domain.xxx, so we will apply a regex expression to filter unwanted formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'site_url' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['site_url'].str.contains(r'^(www\\.)?(.*)?\\/?(.)*')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the 'country' column, the countries are mentioned as a 2 uppercase letter code. Let's confirm every row follows that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'country' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['country'].str.contains(r'[A-Z]{2}')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the column 'domain_rank', which should be a number. Let's verify that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'domain_rank' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'domain_rank':\", fake['domain_rank'].dtypes)\n",
    "print(\"Is it int64?\", fake['domain_rank'].dtypes=='int64')\n",
    "\n",
    "#cast to int64\n",
    "fake['domain_rank'] = fake['domain_rank'].apply(np.int64)\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake=fake[fake['domain_rank']>=0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is \"thread_title\". This column follows the same rules as the 'title' column, it is very hard to define what is a correct title or not. The only verifiable condition is the same as the one used before for 'title'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'thread_title' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['thread_title'].str.contains(r'^\\s*$')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next column is \"spam_score\". This is a very unique column as it only must have float values between 0 and 1. Let's verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'spam_score' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'spam_score':\", fake['spam_score'].dtypes)\n",
    "print(\"Is it float64?\", fake['spam_score'].dtypes == 'float64')\n",
    "\n",
    "#check if all values are greater or equal than 0 and lower or equal than 1\n",
    "fake = fake[fake['spam_score'] >= 0]\n",
    "fake = fake[fake['spam_score'] <= 1]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next column is \"replies_count\" which, as a counter, should be a positive integer or zero. Let's check those conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'replies_count' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'replies_count':\", fake['replies_count'].dtypes)\n",
    "print(\"Is it int64?\", fake['replies_count'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['replies_count'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same verification as \"replies_count\" will happen for the columns \"participants_count\", \"likes\", \"comments\" and \"shares\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'participants_count' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'participants_count':\", fake['participants_count'].dtypes)\n",
    "print(\"Is it int64?\", fake['participants_count'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['participants_count'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'likes' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'likes':\", fake['likes'].dtypes)\n",
    "print(\"Is it int64?\", fake['likes'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['likes'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'comments' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'comments':\", fake['comments'].dtypes)\n",
    "print(\"Is it int64?\", fake['comments'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['comments'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'shares' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "print(\"Type of 'shares':\", fake['shares'].dtypes)\n",
    "print(\"Is it int64?\", fake['shares'].dtypes == 'int64')\n",
    "\n",
    "#check if all values are greater or equal than 0\n",
    "fake = fake[fake['shares'] >= 0]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least we have the \"type\" column, which contains a lowercase string classifying the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> Cleaning 'type' collumn <-\")\n",
    "\n",
    "print(\"Number of rows before: \", len(fake.index))\n",
    "\n",
    "fake = fake[fake['type'].str.contains(r'^[a-z]+$')]  # lowercase letters\n",
    "\n",
    "# clean string rows with just whitespace\n",
    "fake = fake[~fake['type'].str.contains(r'^\\s*$')]\n",
    "\n",
    "print(\"Number of rows after: \", len(fake.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, since the data cleaning is finished, we can save the new clean data to a new csv file, to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.to_csv(\"fake_clean.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63025f8606b7d6daf5c9b2f011dca5c60db3c92c71df34b1c114cc7a6bb35c39"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
